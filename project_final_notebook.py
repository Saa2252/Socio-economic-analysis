# -*- coding: utf-8 -*-
"""Project Final notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AMho0eqvR20sTjkFe_al_8-Jwq2mVC5p
"""



import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns #plotting data
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

#dataset = pd.read_csv('./data/dataset 3.csv')
#dataset[dataset['Target']!='Enrolled'].to_csv('./dataset.csv', index = False)

dataset = pd.read_csv('./dataset.csv')

dataset.head(10).T



"""### Creating a function to convert a categorical column into a One-hot-encoded column"""

def one_hot_encode(df, column_name):

    # Perform one-hot encoding using pandas get_dummies() function
    encoded_df = pd.get_dummies(df[column_name], prefix=column_name)

    # Concatenate the one-hot encoded DataFrame with the original DataFrame
    df = pd.concat([df, encoded_df], axis=1)

    # Drop the original column
    df.drop(column_name, axis=1, inplace=True)

    return df



"""### Exploring the features to keep for modelling
- since we have 4424 rows in the dataset, we will be evaluating individual columns of the dataset and decide which columns are relevant
- we will also have to combine a few social-economic indicators to derive new features

#### Description of the Marital status column
- 1 - single
- 2 - married
- 3 - widower
- 4 - divorced
- 5 - facto union
- 6 - legally separated

For the purpose of simplification of the columns, I am splitting encoding this column into 3 categories:
- Single
- Married
- Others
"""

def marital_status(x):
    if x == 1:
        return 'single'
    elif x == 2:
        return 'married'
    else:
        return 'others'

dataset['Marital status'] = dataset['Marital status'].apply(marital_status)

dataset = dataset.rename(columns={'Marital status': 'marital_status'})

dataset['marital_status'].value_counts()



"""### Application column

 - Application order (between 0 - first choice and 9 last choice)
 - We are categorizing the column values into three classes:
    - Priority 1 : Choices 0-3
    - Priority 2 : Choices 4-6
    - Priority 3 : Choices 7-9

"""

def application_order(val):
    if val in [0,1,2,3]:
        return 'Priority 1'
    elif val in [4,5,6]:
        return 'Priority 2'
    elif val in [7,8,9]:
        return 'Priority 3'

dataset['Application order'] = dataset['Application order'].apply(application_order)

dataset = dataset.rename(columns = {"Application order": "application_order"})

dataset['application_order'].value_counts()



"""### Creating features from the parent qualifications

The dataset has a lot of information about the qualifications and occupations of both the parents
- We will build features to combine the categories into bigger categories for both father and mother

### Encoding qualifications

#### Original Columns
- 1 - Secondary Education - 12th Year of Schooling or Eq.
- 2 - Higher Education - Bachelor's Degree
- 3 - Higher Education - Degree
- 4 - Higher Education - Master's
- 5 - Higher Education - Doctorate
- 6 - Frequency of Higher Education
- 9 - 12th Year of Schooling - Not Completed
- 10 - 11th Year of Schooling - Not Completed
- 11 - 7th Year (Old)
- 12 - Other - 11th Year of Schooling
- 13 - 2nd year complementary high school course
- 14 - 10th Year of Schooling
- 18 - General commerce course
- 19 - Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv.
- 20 - Complementary High School Course
- 22 - Technical-professional course
- 25 - Complementary High School Course - not concluded
- 26 - 7th year of schooling
- 27 - 2nd cycle of the general high school course
- 29 - 9th Year of Schooling - Not Completed
- 30 - 8th year of schooling
- 31 - General Course of Administration and Commerce
- 33 - Supplementary Accounting and Administration
- 34 - Unknown
- 35 - Can't read or write
- 36 - Can read without having a 4th year of schooling
- 37 - Basic education 1st cycle (4th/5th year) or equiv.
- 38 - Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv.
- 39 - Technological specialization course
- 40 - Higher education - degree (1st cycle)
- 41 - Specialized higher studies course
- 42 - Professional higher technical course
- 43 - Higher Education - Master (2nd cycle)
- 44 - Higher Education - Doctorate (3rd cycle)

#### New Coded Features
- Can't read (35, 36)
- Schooling Complete if category (1)
- Schooling incomplete if categories (9, 10, 13, 11, 14, 19, 25, 26, 29, 30, 37, 38)
- Technical education if categories (22, 23, 39, 42)
- Bachelors Course, 1 if categories (2, 40, 41)
- Masters/Phd. Complete, 1 if categories (43, 44, 3, 4, 5)
- Others (34), Any other unselected category

#### Cleaning the Previous Qualification Columns
"""

dataset = dataset.rename(columns = {"Previous qualification": "previous_qualifications"})

def previous_qualification(vals):

    if vals in [9, 10, 12 , 14, 15, 19, 38]:
        return 'Schooling Incomplete'
    elif vals in [1]:
        return 'Schooling Complete'
    elif vals in [39, 40, 42]:
        return 'Technical Education'
    elif vals in [2, 3, 4, 5, 6, 43]:
        return 'Higher Studies'
    else:
        return 'Others'

dataset['previous_qualifications'] = dataset['previous_qualifications'].apply(previous_qualification)\

dataset['previous_qualifications'].value_counts()



def encode_edu_category(vals):
    if vals in [35, 36]:
        return 'Cannot Read/Write'
    elif vals in [11, 25, 26, 29, 30, 37, 38]:
        return 'HS dropout or Below'
    elif vals in [9, 10, 13, 14, 19, 27, 28]:
        return 'HS Completed'
    elif vals in [1]:
        return 'Schooling Complete'
    elif vals in [22, 23, 39, 42]:
        return 'Technical Education Degree'
    elif (vals in [43, 44, 3, 4, 5]) or (vals in [2, 40, 41]):
        return 'Higher Education'
    else:
        return 'Others'

dataset["Mother's qualification"] = dataset["Mother's qualification"].apply(encode_edu_category)

dataset["Father's qualification"] = dataset["Father's qualification"].apply(encode_edu_category)



"""### Renaming the columns
#### Features for Occupation

#### Occupation based feature Categorization
"""

def occupation_based(vals):

    if vals in [1, 2, 122, 10, 123]:
        return 'category 1'
    elif vals in [3, 8, 131, 132, 134, 141, 143, 4]:
        return 'category 2'
    elif vals in [5, 6, 7, 144, 151, 152, 153, 171, 173, 175]:
        return 'category 3'
    elif vals in [191, 192, 193, 194, 9, 0, 90, 99]:
        return 'category 4'
    else:
        return 'Others'

dataset["Father's occupation"] = dataset["Father's occupation"].apply(occupation_based)

dataset["Mother's occupation"] = dataset["Mother's occupation"].apply(occupation_based)



parent_occupation_education_columns = {"Mother's qualification":"M_qual",
                                       "Father's qualification":"F_qual",
                                       "Mother's occupation":"M_occu",
                                       "Father's occupation": "F_occu"}

dataset.rename(columns=parent_occupation_education_columns, inplace = True)

dataset['M_qual'].value_counts()

dataset['F_qual'].value_counts()

dataset['M_occu'].value_counts()

dataset['F_occu'].value_counts()



ren_col_dict1 = {"Tuition fees up to date": "tuition_fee_upto_date"}

dataset = dataset.rename(columns = ren_col_dict1)

"""### Visualizing the Age of Enrollment vs. Target bar plot"""

s1 = dataset[['Age at enrollment', 'Target']].copy()

## create age groups

def age_group(age):

    if age < 20:
        return 'less than 20'
    elif age < 25:
        return '20-25'
    elif age < 30:
        return '25-30'
    elif age < 35:
        return '30-35'
    elif age < 40:
        return '35-40'
    elif age < 45:
        return '40+'

s1['Age_category'] = s1['Age at enrollment'].apply(age_group)

s2 = s1.groupby(['Age_category', 'Target']).count().reset_index(drop=False).rename(columns={'Age at enrollment': 'Count'})

s2

"""####  Plotting the Target by Age of Enrollment"""

import seaborn as sns
sns.set_theme(style="whitegrid")

g = sns.catplot(
    data=s2, kind="bar",
    x="Age_category", y="Count", hue="Target",
    errorbar="sd", palette="dark", alpha=.6, height=6, order=['less than 20', '20-25', '25-30', '30-35', '35-40', '40+']
)
g.despine(left=True)
#g.set_titles("DropOut by Age Group")
g.set_axis_labels("", "Count")
g.legend.set_title("")

dataset.rename(columns = {"Age at enrollment": "enrollment_age"}, inplace=True)

dataset["enrollment_age"] = dataset["enrollment_age"]#.apply(age_group)

#dataset["enrollment_age"].value_counts()

"""#### Conclusion: It can be seen from here that Dropout rate goes up, for higher enrollment age buckets"""





"""### Number of Credits based features Renamed

#### Creating Dictionary of Features with and new names
"""

credits_based_features = ['Curricular units 1st sem (credited)',
                            'Curricular units 1st sem (enrolled)',
                            'Curricular units 1st sem (evaluations)',
                            'Curricular units 1st sem (approved)',
                            'Curricular units 1st sem (grade)',
                            'Curricular units 1st sem (without evaluations)',
                            'Curricular units 2nd sem (credited)',
                            'Curricular units 2nd sem (enrolled)',
                            'Curricular units 2nd sem (evaluations)',
                            'Curricular units 2nd sem (approved)',
                            'Curricular units 2nd sem (grade)',
                            'Curricular units 2nd sem (without evaluations)']

credit_features_renamed = ['1st_sem_crd',
                        '1st_sem_enrl',
                        '1st_sem_eval',
                        '1st_sem_appv',
                        '1st_sem_grade',
                        '1st_sem_w_eval',
                        '2nd_sem_crd',
                        '2nd_sem_enrl',
                        '2nd_sem_eval',
                        '2nd_sem_appv',
                        '2nd_sem_grade',
                        '2nd_sem_w_eval']

renaming_dict_credit_features = {}

for o, n in zip(credits_based_features, credit_features_renamed):
    renaming_dict_credit_features[o] = n


for k, v in zip(list(renaming_dict_credit_features.keys()), list(renaming_dict_credit_features.values())):
    print(k,' -> ', v)


dataset.rename(columns=renaming_dict_credit_features, inplace=True)



"""#### Checking Gender variable's relationship with Target"""

import seaborn as sns

def create_bar_plot_w_categories(df, x_var, y_var, target_var, x_axis_label, order_x = None):

    '''
    df is the dataframe that would include the summarized dataset.
    x_var -> is the column which is being visualized
    y_var -> summarized stat column -> eg. count
    order
    '''

    sns.set_theme(style="whitegrid")

    g = sns.catplot(
        data=df,
        kind="bar",
        x = x_var,
        y = y_var,
        hue=target_var,
        errorbar="sd",
        palette="dark",
        alpha=.6,
        height=6,
        order=order_x
    )
    g.despine(left=True)
    #g.set_titles("DropOut by Age Group")
    g.set_axis_labels("", x_axis_label)
    g.legend.set_title("")



g_t_df = dataset[['Gender', 'Target', 'Nacionality']].copy()
g_t_df.replace({0:'Female', 1: 'Male'}, inplace=True)
gender_df = g_t_df.groupby(['Gender', 'Target'])['Nacionality'].count().reset_index().rename(columns = {'Nacionality':'count'})

create_bar_plot_w_categories(gender_df, 'Gender', 'count', 'Target', 'Count', order_x = ['Male', 'Female'])

"""#  Conclusion: The overall dataset has higher representation of females, yet females show a higher graduate rate."""



"""#### Renaming the remaining columns to maintain consistent naming conventions"""

renamed_colset_2 = {'Unemployment rate':'unemployment_rate',
                    'Inflation rate':'inflation_rate',
                    'GDP':'gdp',
                    'Target':'target',
                    'Course':'course',
                    'Daytime/evening attendance':'daytime_evening_attnd',
                    'M_qual':'m_qual',
                    'F_qual':'f_qual',
                    'M_occu':'m_occ',
                    'F_occu':'f_occ',
                    'Displaced': 'displaced',
                    'Educational special needs': 'special_needs',
                    'Debtor': 'debtor',
                    'Gender':'gender',
                    'Scholarship holder': 'scholarship_holder'}

df = dataset.rename(columns=renamed_colset_2)

dataset_columns = ['marital_status', 'application_order',
       'daytime_evening_attnd', 'previous_qualifications',
       'm_qual', 'f_qual', 'm_occ', 'f_occ', 'displaced', 'special_needs',
       'debtor', 'tuition_fee_upto_date', 'gender', 'scholarship_holder',
       'enrollment_age', '1st_sem_crd', '1st_sem_enrl',
       '1st_sem_eval', '1st_sem_appv', '1st_sem_grade', '1st_sem_w_eval',
       '2nd_sem_crd', '2nd_sem_enrl', '2nd_sem_eval', '2nd_sem_appv',
       '2nd_sem_grade', '2nd_sem_w_eval', 'unemployment_rate',
       'inflation_rate', 'gdp', 'target']


df = df[dataset_columns]

df.head(5)

dataset['previous_qualifications'].value_counts()

"""#### Applying One-hot-Encoding to selected Categorical Columns"""

def one_hot_encode(df, column_names):
    '''
    This is able create one-hot-encodings for the columns passed in as parameters and then drops the columns
    '''
    for column_name in column_names:
        # Perform one-hot encoding using pandas get_dummies() function
        encoded_df = pd.get_dummies(df[column_name], prefix=column_name)

        # Concatenate the one-hot encoded DataFrame with the original DataFrame
        df = pd.concat([df, encoded_df], axis=1)

        # Drop the original column
        df.drop(column_name, axis=1, inplace=True)

        print(column_name, " Encoded and original column dropped")

    return df


cols_to_modify = ['marital_status', 'previous_qualifications', 'm_qual', 'f_qual',
                  'm_occ', 'f_occ', 'application_order']


df1 = one_hot_encode(df, cols_to_modify)

df['marital_status'].value_counts()

"""#### Exploring the Nummeric columns

"""

numeric_cols = ['1st_sem_crd', '1st_sem_enrl', '1st_sem_eval', '1st_sem_appv',
            '1st_sem_grade', '1st_sem_w_eval', '2nd_sem_crd', '2nd_sem_enrl',
            '2nd_sem_eval', '2nd_sem_appv', '2nd_sem_grade', '2nd_sem_w_eval',
            'unemployment_rate', 'inflation_rate', 'gdp']

df[numeric_cols].describe().T



df1.columns

x_cols = ['marital_status_married','marital_status_others', 'marital_status_single',
          'daytime_evening_attnd', 'displaced', 'special_needs',
            'debtor', 'tuition_fee_upto_date', 'gender', 'scholarship_holder',
            '1st_sem_crd', '1st_sem_enrl', '1st_sem_eval', '1st_sem_appv',
            '1st_sem_grade', '1st_sem_w_eval', '2nd_sem_crd', '2nd_sem_enrl',
            '2nd_sem_eval', '2nd_sem_appv', '2nd_sem_grade', '2nd_sem_w_eval',
            'unemployment_rate', 'inflation_rate', 'gdp',
            'previous_qualifications_Higher Studies',
            'previous_qualifications_Others',
            'previous_qualifications_Schooling Complete',
            'previous_qualifications_Schooling Incomplete', 'm_qual_HS Completed',
            'm_qual_HS dropout or Below', 'm_qual_Higher Education',
            'm_qual_Others', 'm_qual_Schooling Complete',
            'm_qual_Technical Education Degree', 'f_qual_HS Completed',
            'f_qual_HS dropout or Below', 'f_qual_Higher Education',
            'f_qual_Others', 'f_qual_Schooling Complete',
            'f_qual_Technical Education Degree', 'm_occ_Others', 'm_occ_category 1',
            'm_occ_category 2', 'm_occ_category 3', 'm_occ_category 4',
            'f_occ_Others', 'f_occ_category 1', 'f_occ_category 2',
            'f_occ_category 3', 'f_occ_category 4', 'application_order_Priority 1',
            'application_order_Priority 2']

X = df1[x_cols]

y = df1['target']



"""### Creating the Target Variable for Model Training step"""

target_replace_dict = {'Graduate': 0,
                        'Dropout': 1,
                        'Enrolled': 2}

y = y.replace(target_replace_dict)

"""#### Exploring and Visualizing the Target labels"""

target_df = pd.DataFrame(df1['target'].value_counts()).reset_index()

target_df.rename(columns = {'index': 'Target',
                            'target': 'Count'}, inplace=True)

sns.barplot(x='Target', y='Count', data=target_df, order=['Graduate', 'Dropout'])
plt.show()

"""#. Conclusion: There is an imbalance in the given dataset, where graduates are over represented in the data.

#### Using the SMOTE to oversample the minority classes
"""

from imblearn.over_sampling import SMOTE

# transform the dataset
oversample = SMOTE()
x_t, target_t = oversample.fit_resample(X, y)

target_t.value_counts()



"""### Implementing Logistic Regression, xgboost, randomForest using 80/20 split"""



from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2)

"""### Building Logistic Regression, Random Forest Classification and XGBoost Algorithms for classification

#### Using Logistic Regression Classifier
"""

lr1 = LogisticRegression()

lr1.fit(X_train, y_train)

y_pred = lr1.predict(X_test)

accuracy_score(y_test, y_pred)

"""#### Using Random Forest Classifier model"""

rf1 = RandomForestClassifier()

rf1.fit(X_train, y_train)

y_pred_rf = rf1.predict(X_test)

accuracy_score(y_test, y_pred_rf)

"""#### Using XGBoost Algorithm for Classification"""

xgb_classifier = xgb.XGBClassifier()

xgb_classifier.fit(X_train, y_train)

y_pred_xgb1 = xgb_classifier.predict(X_test)

accuracy_score(y_test, y_pred_xgb1)

"""## Using K-fold cross validation to perform the data splits

### Since the training set is small, we will train the models after the k-fold data splits
"""

from sklearn.model_selection import cross_val_score, KFold



"""#### Testing for different values of k in k-fold we train and models and assess the accuracies"""

for k in [4, 5, 6, 8, 10]:


    lr2 = LogisticRegression()
    rf2 = RandomForestClassifier()
    xgb2 = xgb.XGBClassifier()

    print("Current value of k:", k)

    lr2_scores = cross_val_score(lr2, X, y, cv = k)
    print(f"Logistic Regression score for k = {k}: ", lr2_scores.mean())

    rf2_scores = cross_val_score(rf2, X, y, cv=k)
    print(f"Random forest score with k = {k} : ", rf2_scores.mean())

    xgb2_scores = cross_val_score(xgb2, X, y, cv = k)
    print(f"XGBoost classification score with k = {k} : ", xgb2_scores.mean())

    print()

"""#### Enhancing the Random-Forest algorithm to fine-tune the model for better performance
 - Performing Grid-search for random-forest model fine tuning while using cross-validation (k=10)
"""

from sklearn.model_selection import GridSearchCV

rf_3 = RandomForestClassifier()

# Define the hyperparameter grid for tuning
param_grid = {
    'n_estimators': [10, 20, 50],
    'max_depth': [None, 5, 10, 12],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

selected_val_k = 10
# Create a K-fold cross-validation object
kfold = KFold(n_splits = selected_val_k, shuffle = True, random_state = 42)

# Perform grid search with cross-validation
grid_search = GridSearchCV(rf_3, param_grid, cv=kfold)
grid_search.fit(X, y)

# Print the best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

grid_search



"""### Retraining the Model using Random Forest Algorithm with best parameter settings - On Train set"""

rf_eval = RandomForestClassifier(max_depth=10,
                                  min_samples_leaf = 4,
                                  min_samples_split = 2,
                                  n_estimators = 50)

rf_eval.fit(X_train, y_train)

"""### Evaluating the Model performance on the Test dataset"""

train_score = accuracy_score(y_train, rf_eval.predict(X_train))
test_score = accuracy_score(y_test, rf_eval.predict(X_test))

print(f"Model Performance on Training set: {round(train_score * 100, 2)}%")
print(f"Model Performance on Test set: {round(test_score * 100, 2)}%")

"""#### Re-training the model to include the entire dataset"""

rf_final = RandomForestClassifier(max_depth=10,
                                  min_samples_leaf = 4,
                                  min_samples_split = 2,
                                  n_estimators = 50)

overall_model_accuracy = accuracy_score(y, rf_final.predict(X))
print(f"Overall model accuracy: {round(overall_model_accuracy * 100, 2)} %", )



"""#### Using the above Model we can assess which input features helped Predicting the target feature"""



fi_df = pd.DataFrame({'feature_names': list(X.columns),
                      'feature_importance': rf_final.feature_importances_})

fi_df.sort_values(by = 'feature_importance', ascending=False, inplace=True)

fi_df

"""#### Creating the Barplot to generate feature importance"""

plt.figure(figsize=(20, 20))
# Create the horizontal plot using Seaborn
sns.barplot(x='feature_importance', y='feature_names', data=fi_df, orient='h')

# Display the plot
plt.title("Feature Importance score of Input features")
plt.show()

"""# Conclusion: The horizontal bar plot shows how different features of the dataset relate with the target.

### Writing the models and the files to disk
"""

fi_df.to_csv('./feature_importance.csv', index = False)

"""#### Writing datasets to file"""

X_train.to_csv('./xtrain.csv', index = False)
X_test.to_csv('./xtest.csv', index = False)
y_train.to_csv('./ytrain.csv', index = False)
y_test.to_csv('./ytest.csv', index = False)



"""#### Writing model file to disk"""

import pickle

# Save the trained model to disk
filename = 'rf_model.pkl'
with open(filename, 'wb') as file:
    pickle.dump(rf_final, file)

print("Model file written to disk")



